import warnings
warnings.filterwarnings("ignore")

import sqlite3
import pandas as pd
import numpy as np
import nltk
import string
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import TfidfVectorizer

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import confusion_matrix
from sklearn import metrics
from sklearn.metrics import roc_curve, auc
from nltk.stem.porter import PorterStemmer

import re
# Tutorial about Python regular expressions: https://pymotw.com/2/re/
import string
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.stem.wordnet import WordNetLemmatizer

from gensim.models import Word2Vec
from gensim.models import KeyedVectors
import pickle
from tqdm import tqdm
import os

"""Reading data"""
twitter_data=pd.read_csv('Sentiment Analysis Dataset.csv',error_bad_lines=False)
twitter_data
print(twitter_data['SentimentText'].values[0])
print("="*50)
print(twitter_data['SentimentText'].values[15])
print("="*50)

"""Stopwords"""

# https://gist.github.com/sebleier/554280
# we are removing the words from the stop words list: 'no', 'nor', 'not'
stopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've",\
            "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \
            'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their',\
            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', \
            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \
            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \
            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\
            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\
            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\
            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \
            's', 't', 'can', 'will', 'just', 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', \
            've', 'y', 'ain', 'aren']

"""Data pre-processing"""

# https://stackoverflow.com/a/47091490/4084039
import re

def decontracted(phrase):
    # specific
    phrase = re.sub(r"won't", "will not", phrase)
    phrase = re.sub(r"can\'t", "can not", phrase)

    # general
    phrase = re.sub("\%", "", phrase)
    phrase = re.sub("\.", "", phrase)
    phrase = re.sub("\&", "", phrase)
    phrase=" ".join(filter(lambda x:x[0]!='@', phrase.split()))
    phrase = re.sub(r"n\'t", " not", phrase)
    phrase = re.sub(r"\'re", " are", phrase)
    phrase = re.sub(r"\'s", " is", phrase)
    phrase = re.sub(r"\'d", " would", phrase)
    phrase = re.sub(r"\'ll", " will", phrase)
    phrase = re.sub(r"\'t", " not", phrase)
    phrase = re.sub(r"\'ve", " have", phrase)
    phrase = re.sub(r"\'m", " am", phrase)
    phrase = phrase.replace('\\r', ' ')
    phrase = phrase.replace('\\"', ' ')
    phrase = phrase.replace('\\n', ' ')
    phrase = " ".join(filter(lambda x:x[0]!='@', phrase.split()))
    phrase = ' '.join(e for e in phrase.split() if e not in stopwords)
    phrase = re.sub('[^A-Za-z0-9]+', ' ', phrase)
    
    return phrase

from tqdm import tqdm
preprocessed_sentiment = []
# tqdm is for printing the status bar
for sentance in tqdm(twitter_data['SentimentText'].values):
    sent = decontracted(sentance)
    sent = ' '.join(e for e in sent.split() if e not in stopwords)
    preprocessed_sentiment.append(sent.lower().strip())

"""Removing words with size <2 and >15 as they does not contain much information."""

twitter_data['processed_SentimentText']=preprocessed_sentiment

twitter_data['processed_SentimentText']= twitter_data['processed_SentimentText'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))
twitter_data['processed_SentimentText']= twitter_data['processed_SentimentText'].apply(lambda x: ' '.join([w for w in x.split() if len(w)<15]))

twitter_data

print(twitter_data['SentimentText'].values[1578598])
print("="*50)
print(twitter_data['SentimentText'].values[1578508])
print("="*50)

print(twitter_data['processed_SentimentText'].values[1578598])
print("="*50)
print(twitter_data['processed_SentimentText'].values[1578508])
print("="*50)

"""Data Analysis"""

len(twitter_data['processed_SentimentText'])

word_list=[]
for sentence in tqdm(range(len(twitter_data['processed_SentimentText']))):
  l=twitter_data['processed_SentimentText'].values[sentence].split()
  for words in l:
    word_list.append(words)

len(word_list)

w=set(word_list)

len(w)

"""From overall tweet dataset finding positive and negative tweets"""

neg_data=twitter_data.loc[twitter_data['Sentiment']==1]

neg_data

neg_data.describe()

pos_data=twitter_data.loc[twitter_data['Sentiment']==0]

pos_data

pos_data.describe()

"""It can be observed that no. of positive sentiment data is almost equal to no.of negative sentiment data. So, it can be said that data is balanced."""

word_list=[]
for sentence in tqdm(range(len(neg_data['processed_SentimentText']))):
  l=neg_data['processed_SentimentText'].values[sentence].split()
  for words in l:
    word_list.append(words)

print("No of words in positive sentiments",len(word_list))
w=set(word_list)
print("No of unique words in positive sentiments",len(w))

from tqdm import tqdm_notebook
D_neg={}
for words in tqdm_notebook(w):
  c=0
  for i in word_list[:10000]:
    if(words==i):
      c=c+1
  D_neg[words]=c

word_list=[]
for sentence in tqdm(range(len(pos_data['processed_SentimentText']))):
  l=pos_data['processed_SentimentText'].values[sentence].split()
  for words in l:
    word_list.append(words)

print("No of words in negative sentiments",len(word_list))
w=set(word_list)
print("No of unique words in negative sentiments",len(w))

from tqdm import tqdm_notebook
D_pos={}
for words in tqdm_notebook(w):
  c=0
  for i in word_list[:10000]:
    if(words==i):
      c=c+1
  D_pos[words]=c

sorted_d_neg = sorted(D_neg.items(), key=lambda kv: kv[1],reverse=True)
sorted_d_pos = sorted(D_pos.items(), key=lambda kv: kv[1],reverse=True)

sorted_d_neg[:5]

sorted_d_pos[:5]

"""Analysing world cloud for positive sentiment"""

from wordcloud import WordCloud 

st =' '.join([sentence for sentence in twitter_data['processed_SentimentText'][twitter_data['Sentiment'] == 0]])

wordcloud = WordCloud(width=1000, height=800, max_font_size=100).generate(st)
plt.figure(figsize=(12, 8))
plt.imshow(wordcloud,interpolation="bilinear")
plt.axis('off')
plt.show()

"""Analysing world cloud for negative sentiment"""

st =' '.join([sentence for sentence in twitter_data['processed_SentimentText'][twitter_data['Sentiment'] == 1]])

wordcloud = WordCloud(width=1000, height=800, max_font_size=100).generate(st)
plt.figure(figsize=(12, 8))
plt.imshow(wordcloud,interpolation="bilinear")
plt.axis('off')
plt.show()

"""From word cloud and other method we are not able to find words which distinguish between positive and negative sentiments because of one word our polarity changes. For eg. 'not' can be used as positively with 'angry' and negatively with 'good' so 'not' word can present in equal amount in both +ve and -ve sentiments.

Analysing twitter sentiment using SentimentIntensityAnalyzer library of nltk toolkit.
"""

import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer

import nltk
nltk.download('vader_lexicon')

sid = SentimentIntensityAnalyzer()
negative=[]
neutral=[]
positive=[]
compound=[]
for i in tqdm(twitter_data['processed_SentimentText']):
    negative.append(sid.polarity_scores(i)['neg'])
    neutral.append(sid.polarity_scores(i)['neu'])
    positive.append(sid.polarity_scores(i)['pos'])
    compound.append(sid.polarity_scores(i)['compound'])
twitter_data['negative']=negative
twitter_data['neutral']=neutral
twitter_data['positive']=positive
twitter_data['compound']=compound

twitter_data

"""Train-test split"""

from sklearn.model_selection import train_test_split
  
# create design matrix X and target vector y
Xsp = (twitter_data.loc[:, twitter_data.columns != 'Sentiment']) # end index is exclusive
ysp = (twitter_data['Sentiment'] )
print(Xsp.shape)
# split the data set into train and test
X_1, X_test, y_1, y_test = train_test_split(Xsp, ysp, test_size=0.33, stratify=ysp)

# split the train data set into cross validation train and cross validation test
X_tr, X_cv, y_tr, y_cv = train_test_split(X_1, y_1, test_size=0.33, stratify=y_1)

print(X_tr.shape, y_tr.shape)
print(X_cv.shape, y_cv.shape)
print(X_test.shape, y_test.shape)

# it returns a dict, keys as class labels and values as the number of data points in that class
train_class_distribution = y_tr.value_counts()
test_class_distribution = y_test.value_counts()
cv_class_distribution = y_cv.value_counts()

my_colors = 'rgbkymc'
train_class_distribution.plot(kind='bar')
plt.xlabel('Class')
plt.ylabel('Data points per Class')
plt.title('Distribution of yi in train data')
plt.grid()
plt.show()

# ref: argsort https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html
# -(train_class_distribution.values): the minus sign will give us in decreasing order
sorted_yi = np.argsort(-train_class_distribution.values)
for i in sorted_yi:
    print('Number of data points in class', i+1, ':',train_class_distribution.values[i], '(', np.round((train_class_distribution.values[i]/y_tr.shape[0]*100), 3), '%)')

    
print('-'*80)
my_colors = 'rgbkymc'
test_class_distribution.plot(kind='bar')
plt.xlabel('Class')
plt.ylabel('Data points per Class')
plt.title('Distribution of yi in test data')
plt.grid()
plt.show()

# ref: argsort https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html
# -(train_class_distribution.values): the minus sign will give us in decreasing order
sorted_yi = np.argsort(-test_class_distribution.values)
for i in sorted_yi:
    print('Number of data points in class', i+1, ':',test_class_distribution.values[i], '(', np.round((test_class_distribution.values[i]/y_test.shape[0]*100), 3), '%)')

print('-'*80)
my_colors = 'rgbkymc'
cv_class_distribution.plot(kind='bar')
plt.xlabel('Class')
plt.ylabel('Data points per Class')
plt.title('Distribution of yi in cross validation data')
plt.grid()
plt.show()

# ref: argsort https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html
# -(train_class_distribution.values): the minus sign will give us in decreasing order
sorted_yi = np.argsort(-train_class_distribution.values)
for i in sorted_yi:
    print('Number of data points in class', i+1, ':',cv_class_distribution.values[i], '(', np.round((cv_class_distribution.values[i]/y_cv.shape[0]*100), 3), '%)')

"""Make Data Model Ready

Processed sentiment text using Bag Of Word model.
"""

vectorizer = CountVectorizer(min_df=10)
Train_sentiment= vectorizer.fit_transform(X_tr['processed_SentimentText'])
CV_sentiment=vectorizer.transform(X_cv['processed_SentimentText'])
Test_sentiment=vectorizer.transform(X_test['processed_SentimentText'])
print(Train_sentiment.shape)
print(CV_sentiment.shape)
print(Test_sentiment.shape)
v5=vectorizer
#print(v5.get_feature_names())

"""Processed sentiment text using Tfidf model."""

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(min_df=10)
Train_sentiment_tfidf = vectorizer.fit_transform(X_tr['processed_SentimentText'])
CV_sentiment_tfidf = vectorizer.transform(X_cv['processed_SentimentText'])
Test_sentiment_tfidf = vectorizer.transform(X_test['processed_SentimentText'])
print(Train_sentiment_tfidf.shape)
print(CV_sentiment_tfidf.shape)
print(Test_sentiment_tfidf.shape)
vt5=vectorizer

from sklearn.externals import joblib
joblib.dump(vt5, 'tfidf.pkl')

"""Processed sentiment text using Word2Vec model."""

i=0
list_of_sentance=[]
for sentnc in tqdm(twitter_data['processed_SentimentText']):
    list_of_sentance.append(sentnc.split())
w2v_model=Word2Vec(list_of_sentance,min_count=5,size=30, workers=4)
w2v_words = list(w2v_model.wv.vocab)
print("number of words that occured minimum 5 times ",len(w2v_words))
print("sample words ", w2v_words[0:50])

print(w2v_model.wv.most_similar('teacher'))
print('='*50)
print(w2v_model.wv.most_similar('student'))

def avg_w2v_essays(text):
    avg_w2v_vectors = []; # the avg-w2v for each sentence/review is stored in this list
    for sentence in tqdm(text): # for each review/sentence
        vector = np.zeros(30) # as word vectors are of zero length
        cnt_words =0; # num of words with a valid vector in the sentence/review
        for word in sentence.split(): # for each word in a review/sentence
            if word in w2v_model:
                vector += w2v_model.wv[word]
                cnt_words += 1
                cnt_words += 1
        if cnt_words != 0:
            vector /= cnt_words
        avg_w2v_vectors.append(vector)
    return(avg_w2v_vectors)
Train_Sentiment_w2v=avg_w2v_essays(X_tr['processed_SentimentText'])
CV_Sentiment_w2v=avg_w2v_essays(X_cv['processed_SentimentText'])
Test_Sentiment_w2v=avg_w2v_essays(X_test['processed_SentimentText'])
print(len(Train_Sentiment_w2v))
print(len(CV_Sentiment_w2v))
print(len(Test_Sentiment_w2v))

"""Processed sentiment text using Tfidf weighted word2vec"""

def essay_tfidf_w2v(text,tfidf_model,dictionary,tfidf_words):
    
    tfidf_model.transform(text)
    
    tfidf_w2v_vectors = []; # the avg-w2v for each sentence/review is stored in this list
    for sentence in tqdm(text): # for each review/sentence
        vector = np.zeros(30) # as word vectors are of zero length
        tf_idf_weight =0; # num of words with a valid vector in the sentence/review
        for word in sentence.split(): # for each word in a review/sentence
            if (word in w2v_model) and (word in tfidf_words):
                vec = w2v_model.wv[word] # getting the vector for each word
                # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))
                tf_idf = dictionary[word]*(sentence.count(word)/len(sentence.split())) # getting the tfidf value for each word
                vector += (vec * tf_idf) # calculating tfidf weighted w2v
                tf_idf_weight += tf_idf
        if tf_idf_weight != 0:
            vector /= tf_idf_weight
        tfidf_w2v_vectors.append(vector)
    return(tfidf_w2v_vectors)
tfidf_model = TfidfVectorizer()
tfidf_model.fit(X_tr['processed_SentimentText'])
# we are converting a dictionary with word as a key, and the idf as a value

dictionary = dict(zip(tfidf_model.get_feature_names(), list(tfidf_model.idf_)))
tfidf_words = set(tfidf_model.get_feature_names())
Train_Sentiment_tfidf_w2v=essay_tfidf_w2v(X_tr['processed_SentimentText'],tfidf_model,dictionary,tfidf_words)
CV_Sentiment_tfidf_w2v=essay_tfidf_w2v(X_cv['processed_SentimentText'],tfidf_model,dictionary,tfidf_words)
Test_Sentiment_tfidf_w2v=essay_tfidf_w2v(X_test['processed_SentimentText'],tfidf_model,dictionary,tfidf_words)
print(len(Train_Sentiment_tfidf_w2v))
print(len(CV_Sentiment_tfidf_w2v))
print(len(Test_Sentiment_tfidf_w2v))

"""Analysing positive sentiments."""

from sklearn.preprocessing import Normalizer

norm = Normalizer()
Train_e_pos_word = norm.fit_transform(X_tr['positive'].values.reshape(-1, 1))
CV_e_pos_word=norm.transform(X_cv['positive'].values.reshape(-1, 1))                           
Test_e_pos_word=norm.transform(X_test['positive'].values.reshape(-1, 1))                            
print('Training data shape',Train_e_pos_word.shape)
print('cv data shape',CV_e_pos_word.shape)
print('Test data shape',Test_e_pos_word.shape)

"""Analysing negative sentiments."""

norm = Normalizer()
Train_e_neg_word = norm.fit_transform(X_tr['negative'].values.reshape(-1, 1))
CV_e_neg_word=norm.transform(X_cv['negative'].values.reshape(-1, 1))                           
Test_e_neg_word=norm.transform(X_test['negative'].values.reshape(-1, 1))                            
print('Training data shape',Train_e_neg_word.shape)
print('cv data shape',CV_e_neg_word.shape)
print('Test data shape',Test_e_neg_word.shape)

"""Analysing neutral sentiments."""

norm = Normalizer()
Train_e_neu_word = norm.fit_transform(X_tr['neutral'].values.reshape(-1, 1))
CV_e_neu_word=norm.transform(X_cv['neutral'].values.reshape(-1, 1))                           
Test_e_neu_word=norm.transform(X_test['neutral'].values.reshape(-1, 1))                            
print('Training data shape',Train_e_neu_word.shape)
print('cv data shape',CV_e_neu_word.shape)
print('Test data shape',Test_e_neu_word.shape)

"""Analysing compound sentiments."""

norm = Normalizer()
Train_e_comp_word = norm.fit_transform(X_tr['compound'].values.reshape(-1, 1))
CV_e_comp_word=norm.transform(X_cv['compound'].values.reshape(-1, 1))                           
Test_e_comp_word=norm.transform(X_test['compound'].values.reshape(-1, 1))                            
print('Training data shape',Train_e_comp_word.shape)
print('cv data shape',CV_e_comp_word.shape)
print('Test data shape',Test_e_comp_word.shape)

import matplotlib.pyplot as plt
from scipy.sparse import hstack
from sklearn.manifold import TSNE
from sklearn import datasets, neighbors
from matplotlib.colors import ListedColormap
from mlxtend.plotting import plot_decision_regions

"""Compound sentiment using BOW."""

def batch_predict(clf, data):
    # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class
    # not the predicted outputs

    y_data_pred = []
    tr_loop = data.shape[0] - data.shape[0]%1000
    # consider you X_tr shape is 49041, then your cr_loop will be 49041 - 49041%1000 = 49000
    # in this for loop we will iterate unti the last 1000 multiplier
    for i in range(0, tr_loop, 1000):
        y_data_pred.extend(clf.predict_proba(data[i:i+1000])[:,1])
    # we will be predicting for the last data points
    y_data_pred.extend(clf.predict_proba(data[tr_loop:])[:,1])
    
    return y_data_pred

import math
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import roc_auc_score

train_auc = []
cv_auc = []
K = {'C': [10**-4, 10**-2, 10**0, 10**2, 10**4]}
for i in tqdm(K['C']):
    neigh = LogisticRegression(C=i,penalty='l2')
    neigh.fit(Train_sentiment, y_tr)

    y_train_pred = batch_predict(neigh, Train_sentiment)    
    y_cv_pred = batch_predict(neigh, CV_sentiment)

    # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class
    # not the predicted outputs        
    train_auc.append(roc_auc_score(y_tr,y_train_pred))
    cv_auc.append(roc_auc_score(y_cv, y_cv_pred))
log_K=[]
for l in K['C']:
    log_K.append(math.log(l))
plt.plot(log_K, train_auc, label='Train AUC')
plt.plot(log_K, cv_auc, label='CV AUC')

plt.scatter(log_K, train_auc, label='Train AUC points')
plt.scatter(log_K, cv_auc, label='CV AUC points')

plt.legend()
plt.xlabel("K: hyperparameter")
plt.ylabel("AUC")
plt.title("ERROR PLOTS")
plt.grid()
plt.show()

best_par1=1

from sklearn.metrics import roc_curve, auc

neigh = LogisticRegression(C=best_par1,penalty='l2')
neigh.fit(Train_sentiment, y_tr)
# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class
# not the predicted outputs

y_train_pred = batch_predict(neigh, Train_sentiment)    
y_test_pred = batch_predict(neigh, Test_sentiment)

train_fpr, train_tpr, tr_thresholds = roc_curve(y_tr, y_train_pred)
test_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)

plt.plot(train_fpr, train_tpr, label="train AUC ="+str(auc(train_fpr, train_tpr)))
plt.plot(test_fpr, test_tpr, label="test AUC ="+str(auc(test_fpr, test_tpr)))
plt.legend()
plt.xlabel("K: hyperparameter")
plt.ylabel("AUC")
plt.title("ERROR PLOTS")
plt.grid()
plt.show()

def predict(proba, threshould, fpr, tpr):
    
    t = threshould[np.argmax(tpr*(1-fpr))]
    
    # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high
    
    print("the maximum value of tpr*(1-fpr)", max(tpr*(1-fpr)), "for threshold", np.round(t,3))
    predictions = []
    for i in proba:
        if i>=t:
            predictions.append(1)
        else:
            predictions.append(0)
    return predictions

import seaborn as sns
print("Train confusion matrix")
sns.heatmap(confusion_matrix(y_tr, predict(y_train_pred, tr_thresholds, train_fpr, train_fpr)), annot=True, fmt='g')
plt.xlabel("Predicted")
plt.ylabel("Actual")

"""Compound sentiment using Tfidf"""

train_auc = []
cv_auc = []
K = {'C': [10**-4, 10**-2, 10**0, 10**2, 10**4]}
for i in tqdm(K['C']):
    neigh = LogisticRegression(C=i,penalty='l2')
    neigh.fit(Train_sentiment_tfidf, y_tr)

    y_train_pred = batch_predict(neigh, Train_sentiment_tfidf)    
    y_cv_pred = batch_predict(neigh, CV_sentiment_tfidf)

    # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class
    # not the predicted outputs        
    train_auc.append(roc_auc_score(y_tr,y_train_pred))
    cv_auc.append(roc_auc_score(y_cv, y_cv_pred))
log_K=[]
for l in K['C']:
    log_K.append(math.log(l))
plt.plot(log_K, train_auc, label='Train AUC')
plt.plot(log_K, cv_auc, label='CV AUC')

plt.scatter(log_K, train_auc, label='Train AUC points')
plt.scatter(log_K, cv_auc, label='CV AUC points')

plt.legend()
plt.xlabel("K: hyperparameter")
plt.ylabel("AUC")
plt.title("ERROR PLOTS")
plt.grid()
plt.show()

best_par1=1

from sklearn.metrics import roc_curve, auc

neigh = LogisticRegression(C=best_par1,penalty='l2')
neigh.fit(Train_sentiment_tfidf, y_tr)
# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class
# not the predicted outputs

y_train_pred = batch_predict(neigh, Train_sentiment_tfidf)    
y_test_pred = batch_predict(neigh, Test_sentiment_tfidf)

train_fpr, train_tpr, tr_thresholds = roc_curve(y_tr, y_train_pred)
test_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)

plt.plot(train_fpr, train_tpr, label="train AUC ="+str(auc(train_fpr, train_tpr)))
plt.plot(test_fpr, test_tpr, label="test AUC ="+str(auc(test_fpr, test_tpr)))
plt.legend()
plt.xlabel("K: hyperparameter")
plt.ylabel("AUC")
plt.title("ERROR PLOTS")
plt.grid()
plt.show()

joblib.dump(neigh, 'tflr.pkl')

import seaborn as sns
print("Train confusion matrix")
sns.heatmap(confusion_matrix(y_tr, predict(y_train_pred, tr_thresholds, train_fpr, train_fpr)), annot=True, fmt='g')
plt.xlabel("Predicted")
plt.ylabel("Actual")

"""Compound sentiment using Word2Vec."""

train_auc = []
cv_auc = []
K = {'C': [10**-4, 10**-2, 10**0, 10**2, 10**4]}
for i in tqdm(K['C']):
    neigh = LogisticRegression(C=i,penalty='l2')
    neigh.fit(Train_Sentiment_w2v, y_tr)

    y_train_pred = neigh.predict(Train_Sentiment_w2v)    
    y_cv_pred = neigh.predict(CV_Sentiment_w2v)

    # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class
    # not the predicted outputs        
    train_auc.append(roc_auc_score(y_tr,y_train_pred))
    cv_auc.append(roc_auc_score(y_cv, y_cv_pred))
log_K=[]
for l in K['C']:
    log_K.append(math.log(l))
plt.plot(log_K, train_auc, label='Train AUC')
plt.plot(log_K, cv_auc, label='CV AUC')

plt.scatter(log_K, train_auc, label='Train AUC points')
plt.scatter(log_K, cv_auc, label='CV AUC points')

plt.legend()
plt.xlabel("K: hyperparameter")
plt.ylabel("AUC")
plt.title("ERROR PLOTS")
plt.grid()
plt.show()

best_par1=0.01

from sklearn.metrics import roc_curve, auc

neigh = LogisticRegression(C=best_par1,penalty='l2',class_weight='balanced')
neigh.fit(Train_Sentiment_w2v, y_tr)
# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class
# not the predicted outputs

y_train_pred = neigh.predict(Train_Sentiment_w2v)    
y_test_pred = neigh.predict(Test_Sentiment_w2v)

train_fpr, train_tpr, tr_thresholds = roc_curve(y_tr, y_train_pred)
test_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)

plt.plot(train_fpr, train_tpr, label="train AUC ="+str(auc(train_fpr, train_tpr)))
plt.plot(test_fpr, test_tpr, label="test AUC ="+str(auc(test_fpr, test_tpr)))
plt.legend()
plt.xlabel("K: hyperparameter")
plt.ylabel("AUC")
plt.title("ERROR PLOTS")
plt.grid()
plt.show()

import seaborn as sns
print("Train confusion matrix")
sns.heatmap(confusion_matrix(y_tr, predict(y_train_pred, tr_thresholds, train_fpr, train_fpr)), annot=True, fmt='g')
plt.xlabel("Predicted")
plt.ylabel("Actual")

"""Compound sentiment using tfidf weighted w2v"""

train_auc = []
cv_auc = []
K = {'C': [10**-4, 10**-2, 10**0, 10**1]}
for i in tqdm(K['C']):
    neigh = LogisticRegression(C=i,penalty='l2')
    neigh.fit(Train_Sentiment_tfidf_w2v, y_tr)

    y_train_pred = neigh.predict(Train_Sentiment_tfidf_w2v)    
    y_cv_pred = neigh.predict(CV_Sentiment_tfidf_w2v)

    # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class
    # not the predicted outputs        
    train_auc.append(roc_auc_score(y_tr,y_train_pred))
    cv_auc.append(roc_auc_score(y_cv, y_cv_pred))
log_K=[]
for l in K['C']:
    log_K.append(math.log(l))
plt.plot(log_K, train_auc, label='Train AUC')
plt.plot(log_K, cv_auc, label='CV AUC')

plt.scatter(log_K, train_auc, label='Train AUC points')
plt.scatter(log_K, cv_auc, label='CV AUC points')

plt.legend()
plt.xlabel("K: hyperparameter")
plt.ylabel("AUC")
plt.title("ERROR PLOTS")
plt.grid()
plt.show()

best_par1=0.01

from sklearn.metrics import roc_curve, auc

neigh = LogisticRegression(C=best_par1,penalty='l2')
neigh.fit(Train_Sentiment_tfidf_w2v, y_tr)
# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class
# not the predicted outputs

y_train_pred = neigh.predict( Train_Sentiment_tfidf_w2v)    
y_test_pred = neigh.predict(Test_Sentiment_tfidf_w2v)

train_fpr, train_tpr, tr_thresholds = roc_curve(y_tr, y_train_pred)
test_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)

plt.plot(train_fpr, train_tpr, label="train AUC ="+str(auc(train_fpr, train_tpr)))
plt.plot(test_fpr, test_tpr, label="train AUC ="+str(auc(test_fpr, test_tpr)))
plt.legend()
plt.xlabel("K: hyperparameter")
plt.ylabel("AUC")
plt.title("ERROR PLOTS")
plt.grid()
plt.show()

import seaborn as sns
print("Train confusion matrix")
sns.heatmap(confusion_matrix(y_tr, predict(y_train_pred, tr_thresholds, train_fpr, train_fpr)), annot=True, fmt='g')
plt.xlabel("Predicted")
plt.ylabel("Actual")

"""From all the above models LR perform good in Tfidf.

Now, analysing sentiment using tfidf.
"""

Xh5 = hstack((Train_sentiment_tfidf,Train_e_pos_word,Train_e_neg_word,Train_e_neu_word,Train_e_comp_word)).tocsr()

Xh5_test=hstack((Test_sentiment_tfidf,Test_e_pos_word,Test_e_neg_word,Test_e_neu_word,Test_e_comp_word)).tocsr()

Xh5_cross=hstack((CV_sentiment_tfidf,CV_e_pos_word,CV_e_neg_word,CV_e_neu_word,CV_e_comp_word)).tocsr()

train_auc = []
cv_auc = []
K = {'C': [10**-4, 10**-2, 10**0, 10**2, 10**4]}
for i in tqdm(K['C']):
    neigh = LogisticRegression(C=i,penalty='l2')
    neigh.fit(Xh5, y_tr)

    y_train_pred = batch_predict(neigh, Xh5)    
    y_cv_pred = batch_predict(neigh, Xh5_cross)

    # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class
    # not the predicted outputs        
    train_auc.append(roc_auc_score(y_tr,y_train_pred))
    cv_auc.append(roc_auc_score(y_cv, y_cv_pred))
log_K=[]
for l in K['C']:
    log_K.append(math.log(l))
plt.plot(log_K, train_auc, label='Train AUC')
plt.plot(log_K, cv_auc, label='CV AUC')

plt.scatter(log_K, train_auc, label='Train AUC points')
plt.scatter(log_K, cv_auc, label='CV AUC points')

plt.legend()
plt.xlabel("K: hyperparameter")
plt.ylabel("AUC")
plt.title("ERROR PLOTS")
plt.grid()
plt.show()

best_par1=1

from sklearn.metrics import roc_curve, auc

neigh = LogisticRegression(C=best_par1,penalty='l2',class_weight='balanced')
neigh.fit(Xh5, y_tr)
# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class
# not the predicted outputs

y_train_pred = batch_predict(neigh, Xh5)    
y_test_pred = batch_predict(neigh, Xh5_test)

train_fpr, train_tpr, tr_thresholds = roc_curve(y_tr, y_train_pred)
test_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)

plt.plot(train_fpr, train_tpr, label="train AUC ="+str(auc(train_fpr, train_tpr)))
plt.plot(test_fpr, test_tpr, label="train AUC ="+str(auc(test_fpr, test_tpr)))
plt.legend()
plt.xlabel("K: hyperparameter")
plt.ylabel("AUC")
plt.title("ERROR PLOTS")
plt.grid()
plt.show()

import pickle
joblib.dump(neigh, 'lr.pkl')

import seaborn as sns
print("Train confusion matrix")
sns.heatmap(confusion_matrix(y_tr, predict(y_train_pred, tr_thresholds, train_fpr, train_fpr)), annot=True, fmt='g')
plt.xlabel("Predicted")
plt.ylabel("Actual")

from prettytable import PrettyTable

x = PrettyTable()

x.field_names = ["Model","Vectorizer", "Hyper parameter", "Train AUC","Test AUC"]

x.add_row(["Logistic Regression","BOW", "C=1",0.87282,85719 ])
x.add_row(["Logistic Regression","TFIDF", "C=1",0.87501,0.86423 ])
x.add_row(["Logistic Regression","W2V", "C=0.01",0.73759,0.73758 ])
x.add_row(["Logistic Regression","TFIDF W2V", "C=0.01",0.73008,0.73049 ])
x.add_row(["Logistic Regression","TFIDF W2V + sentiments", "C=1",0.87658,0.86604 ])

print(x)